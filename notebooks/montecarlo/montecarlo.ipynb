{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Bayesian Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Introduction\n",
    "\n",
    "For most problems of interest, Bayesian analysis requires integration over multiple parameters, making the calculation of a [posterior](https://en.wikipedia.org/wiki/Posterior_probability) intractable whether via analytic methods or standard methods of numerical integration.\n",
    "\n",
    "However, it is often possible to *approximate* these integrals by drawing samples\n",
    "from posterior distributions. For example, consider the expected value (mean) of a vector-valued random variable $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "E[\\mathbf{x}] = \\int \\mathbf{x} f(\\mathbf{x}) \\mathrm{d}\\mathbf{x}\\,, \\quad\n",
    "\\mathbf{x} = \\{x_1, \\ldots, x_k\\}\n",
    "$$\n",
    "\n",
    "where $k$ (dimension of vector $\\mathbf{x}$) is perhaps very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can produce a reasonable number of random vectors $\\{{\\bf x_i}\\}$, we can use these values to approximate the unknown integral. This process is known as [**Monte Carlo integration**](https://en.wikipedia.org/wiki/Monte_Carlo_integration). In general, Monte Carlo integration allows integrals against probability density functions\n",
    "\n",
    "$$\n",
    "I = \\int h(\\mathbf{x}) f(\\mathbf{x}) \\mathrm{d}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "to be estimated by finite sums\n",
    "\n",
    "$$\n",
    "\\hat{I} = \\frac{1}{n}\\sum_{i=1}^n h(\\mathbf{x}_i),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is a sample from $f$. This estimate is valid and useful because:\n",
    "\n",
    "- $\\hat{I} \\rightarrow I$ with probability $1$ by the [strong law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law);\n",
    "\n",
    "- simulation error can be measured and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (Negative Binomial Distribution)\n",
    "\n",
    "We can use this kind of simulation to estimate the expected value of a random variable that is negative binomial-distributed. The [negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution) applies to discrete positive random variables. It can be used to model the number of Bernoulli trials that one can expect to conduct until $r$ failures occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) reads\n",
    "\n",
    "$$\n",
    "f(k \\mid p, r) = {k + r - 1 \\choose k} (1 - p)^k p^r\\,,\n",
    "$$\n",
    "\n",
    "where $k \\in \\{0, 1, 2, \\ldots \\}$ is the value taken by our positive random variable and\n",
    "$p$ is the probability of success ($0 < p < 1$).\n",
    "\n",
    "\n",
    "![negative binomial (courtesy Wikipedia)](http://upload.wikimedia.org/wikipedia/commons/8/83/Negbinomial.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequently, this distribution is used to model *overdispersed counts*, that is, counts that have variance larger than what would be predicted under a Poisson distribution.\n",
    "In fact, the negative binomial can be expressed as a continuous mixture of Poisson distributions, where a Gamma distribution acts as the mixing distribution.\n",
    "\n",
    "$$\n",
    "f(k \\mid p, r) = \\int_0^{\\infty} \\text{Poisson}(k \\mid \\lambda) \\,\n",
    "\\text{Gamma}(r, (1 - p)/p) \\, \\mathrm{d}\\lambda\n",
    "$$\n",
    "\n",
    "Let's resort to simulation to estimate the mean of a negative binomial distribution with $p = 0.7$ and $r = 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = 3\n",
    "p = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate Gamma means\n",
    "lam = np.random.gamma(r, p / (1 - p), size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3899999999999997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate sample Poisson conditional on lambda\n",
    "sim_vals = np.random.poisson(lam)\n",
    "sim_vals.sum() / 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual expected value of the negative binomial distribution is \\\\(p r / (1-p)\\\\), which in this case is 7. That's pretty close, though we can do better if we draw more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9849899999999998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam = np.random.gamma(r, p/(1.-p), size=100000)\n",
    "sim_vals = np.random.poisson(lam)\n",
    "sim_vals.sum() / 100000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach of drawing repeated random samples in order to obtain a desired numerical result is generally known as **Monte Carlo simulation**.\n",
    "\n",
    "Clearly, this is a convenient, simplistic example that did not require simuation to obtain an answer. For most problems, it is simply not possible to draw independent random samples from the posterior distribution because they will generally be (1) multivariate and (2) not of a known functional form for which there is a pre-existing random number generator.\n",
    "\n",
    "However, we are not going to give up on simulation. Though we cannot generally draw independent samples for our model, we can usually generate **dependent** samples, and it turns out that if we do this in a particular way, we can obtain samples from almost any posterior distribution.\n",
    "\n",
    "## Markov Chains\n",
    "\n",
    "A Markov chain is a special type of *stochastic process*. The standard definition of a stochastic process is an ordered collection of random variables:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\begin{split}\\{X_t:t \\in T\\}\\end{split}\\notag\\\\\\begin{split}\\end{split}\\notag\n",
    "\\end{gathered}$$\n",
    "\n",
    "where $t$ is frequently (but not necessarily) a time index. If we think of $X_t$ as a state $X$ at time $t$, and invoke the following dependence condition on each state:\n",
    "\n",
    "\\\\[\\begin{aligned}\n",
    "&Pr(X_{t+1}=x_{t+1} | X_t=x_t, X_{t-1}=x_{t-1},\\ldots,X_0=x_0) \\cr\n",
    "&= Pr(X_{t+1}=x_{t+1} | X_t=x_t)\n",
    "\\end{aligned}\\\\]\n",
    "\n",
    "then the stochastic process is known as a Markov chain. This conditioning specifies that the future depends on the current state, but not past states. Thus, the Markov chain wanders about the state space,\n",
    "remembering only where it has just been in the last time step. \n",
    "\n",
    "The collection of transition probabilities is sometimes called a *transition matrix* when dealing with discrete states, or more generally, a *transition kernel*.\n",
    "\n",
    "It is useful to think of the Markovian property as **mild non-independence**. \n",
    "\n",
    "If we use Monte Carlo simulation to generate a Markov chain, this is called **Markov chain Monte Carlo**, or MCMC. If the resulting Markov chain obeys some important properties, then it allows us to indirectly generate independent samples from a particular posterior distribution.\n",
    "\n",
    "\n",
    "> ### Why MCMC Works: Reversible Markov Chains\n",
    "> \n",
    "> Markov chain Monte Carlo simulates a Markov chain for which some function of interest\n",
    "> (*e.g.* the joint distribution of the parameters of some model) is the unique, invariant limiting distribution. An invariant distribution with respect to some Markov chain with transition kernel $Pr(y \\mid x)$ implies that:\n",
    "> \n",
    "> $$\\int_x Pr(y \\mid x) \\pi(x) dx = \\pi(y).$$\n",
    "> \n",
    "> Invariance is guaranteed for any *reversible* Markov chain. Consider a Markov chain in reverse sequence:\n",
    "> $\\{\\theta^{(n)},\\theta^{(n-1)},...,\\theta^{(0)}\\}$. This sequence is still Markovian, because:\n",
    "> \n",
    "> $$Pr(\\theta^{(k)}=y \\mid \\theta^{(k+1)}=x,\\theta^{(k+2)}=x_1,\\ldots ) = Pr(\\theta^{(k)}=y \\mid \\theta^{(k+1)}=x)$$\n",
    "> \n",
    "> Forward and reverse transition probabilities may be related through Bayes theorem:\n",
    "> \n",
    "> $$\\frac{Pr(\\theta^{(k+1)}=x \\mid \\theta^{(k)}=y) \\pi^{(k)}(y)}{\\pi^{(k+1)}(x)}$$\n",
    "> \n",
    "> Though not homogeneous in general, $\\pi$ becomes homogeneous if:\n",
    "> \n",
    "> -   $n \\rightarrow \\infty$\n",
    "> \n",
    "> -   $\\pi^{(i)}=\\pi$ for some $i < k$\n",
    "> \n",
    "> If this chain is homogeneous it is called reversible, because it satisfies the ***detailed balance equation***:\n",
    "> \n",
    "> $$\\pi(x)Pr(y \\mid x) = \\pi(y) Pr(x \\mid y)$$\n",
    "> \n",
    "> Reversibility is important because it has the effect of balancing movement through the entire state space. When a Markov chain is reversible, $\\pi$ is the unique, invariant, stationary distribution of that chain. Hence, if $\\pi$ is of interest, we need only find the reversible Markov chain for which $\\pi$ is the limiting distribution.\n",
    "> This is what MCMC does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling\n",
    "\n",
    "The Gibbs sampler is the simplest and most prevalent MCMC algorithm. If a posterior has $k$ parameters to be estimated, we may condition each parameter on current values of the other $k-1$ parameters, and sample from the resultant distributional form (usually easier), and repeat this operation on the other parameters in turn. This procedure generates samples from the posterior distribution. Note that we have now combined Markov chains (conditional independence) and Monte Carlo techniques (estimation by simulation) to yield Markov chain Monte Carlo.\n",
    "\n",
    "Here is a stereotypical Gibbs sampling algorithm:\n",
    "\n",
    "1.  Choose starting values for states (parameters):\n",
    "    ${\\bf \\theta} = [\\theta_1^{(0)},\\theta_2^{(0)},\\ldots,\\theta_k^{(0)}]$\n",
    "\n",
    "2.  Initialize counter $j=1$\n",
    "\n",
    "3.  Draw the following values from each of the $k$ conditional\n",
    "    distributions:\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "    \\theta_1^{(j)} &\\sim& \\pi(\\theta_1 | \\theta_2^{(j-1)},\\theta_3^{(j-1)},\\ldots,\\theta_{k-1}^{(j-1)},\\theta_k^{(j-1)}) \\\\\n",
    "    \\theta_2^{(j)} &\\sim& \\pi(\\theta_2 | \\theta_1^{(j)},\\theta_3^{(j-1)},\\ldots,\\theta_{k-1}^{(j-1)},\\theta_k^{(j-1)}) \\\\\n",
    "    \\theta_3^{(j)} &\\sim& \\pi(\\theta_3 | \\theta_1^{(j)},\\theta_2^{(j)},\\ldots,\\theta_{k-1}^{(j-1)},\\theta_k^{(j-1)}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{k-1}^{(j)} &\\sim& \\pi(\\theta_{k-1} | \\theta_1^{(j)},\\theta_2^{(j)},\\ldots,\\theta_{k-2}^{(j)},\\theta_k^{(j-1)}) \\\\\n",
    "    \\theta_k^{(j)} &\\sim& \\pi(\\theta_k | \\theta_1^{(j)},\\theta_2^{(j)},\\theta_4^{(j)},\\ldots,\\theta_{k-2}^{(j)},\\theta_{k-1}^{(j)})\\end{aligned}$$\n",
    "\n",
    "4.  Increment $j$ and repeat until convergence occurs.\n",
    "\n",
    "As we can see from the algorithm, each distribution is conditioned on the last iteration of its chain values, constituting a Markov chain as advertised. The Gibbs sampler has all of the important properties outlined in the previous section: it is aperiodic, homogeneous and ergodic. Once the sampler converges, all subsequent samples are from the target distribution. This convergence occurs at a geometric rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inferring patterns in UK coal mining disasters\n",
    "\n",
    "Let's try to model a more interesting example, a time series of recorded coal mining \n",
    "disasters in the UK from 1851 to 1962.\n",
    "\n",
    "Occurrences of disasters in the time series is thought to be derived from a \n",
    "Poisson process with a large rate parameter in the early part of the time \n",
    "series, and from one with a smaller rate in the later part. We are interested \n",
    "in locating the change point in the series, which perhaps is related to changes \n",
    "in mining safety regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disasters_array = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "                         3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "                         2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "                         1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "                         0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "                         3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "                         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "n_count_data = len(disasters_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/anaconda/lib/python2.7/site-packages/pytz/__init__.py:29: UserWarning: Module argparse was already imported from /Users/jack/anaconda/lib/python2.7/argparse.pyc, but /Users/jack/anaconda/lib/python2.7/site-packages is being added to sys.path\n",
      "  from pkg_resources import resource_stream\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6352.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "data = Data([\n",
    "    Bar(\n",
    "        x=np.arange(1851, 1962),\n",
    "        y=disasters_array\n",
    "    )\n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    title=\"UK coal mining disasters, 1851-1962\",\n",
    "    xaxis=XAxis(\n",
    "        title='Year'\n",
    "    ),\n",
    "    yaxis=YAxis(\n",
    "        title='Disasters'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='disasters_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use [Poisson](http://en.wikipedia.org/wiki/Poisson_distribution) random variables for this type of count data. Denoting year $i$'s accident count by $y_i$, \n",
    "\n",
    "$$ y_i \\sim \\text{Poisson}(\\lambda) $$\n",
    "\n",
    "For those unfamiliar, Poisson random variables look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6353.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.tools as tls\n",
    "\n",
    "data = Data([Histogram(x=np.random.poisson(l, 1000), opacity=0.75, name=u'λ=%i' % l) for l in [1, 5, 12, 25]])\n",
    "layout = Layout(\n",
    "    barmode='overlay',\n",
    "    title='Poisson means'\n",
    ")\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='poisson_means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling problem revolves around estimating the values of the $\\lambda$ parameters. Looking at the time series above, it appears that the rate declines later in the time series.\n",
    "\n",
    "A ***changepoint model*** identifies a point (year) during the observation period (call it $\\tau$) after which the parameter $\\lambda$ drops to a lower value. So we are estimating two $\\lambda$ parameters: one for the early period and another for the late period.\n",
    "\n",
    "$$\n",
    "\\lambda = \n",
    "\\begin{cases}\n",
    "\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We need to assign prior probabilities to both $\\lambda$ parameters. The [gamma distribution](http://en.wikipedia.org/wiki/Gamma_distribution) not only provides a continuous density function for positive numbers, but it is also **conjugate** with the Poisson sampling distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6354.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_params = [(0.1, 100), (1, 100), (1, 10), (10, 10)]\n",
    "data = Data([Histogram(x=np.random.gamma(*p, size=1000), \n",
    "                       opacity=0.75, \n",
    "                       name=u'α=%i, β=%i' % (p[0], p[1])) for p in gamma_params])\n",
    "\n",
    "layout = Layout(\n",
    "    barmode='overlay',\n",
    "    xaxis=XAxis(\n",
    "        range=[0, 300]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='gamma_distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will specify suitably vague hyperparameters $\\alpha$ and $\\beta$ for both priors.\n",
    "\n",
    "\\begin{align}\n",
    "&\\lambda_1 \\sim \\text{Gamma}( 1, 10 ) \\\\\\\n",
    "&\\lambda_2 \\sim \\text{Gamma}( 1, 10 )\n",
    "\\end{align}\n",
    "\n",
    "Since we do not have any intuition about the location of the changepoint (prior to viewing the data), we will assign a discrete uniform prior over all years 1851-1962.\n",
    "\n",
    "\\begin{align}\n",
    "& \\tau \\sim \\text{DiscreteUniform(1851,1962) }\\\\\\\\\n",
    "& \\Rightarrow P( \\tau = k ) = \\frac{1}{111}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gibbs sampling\n",
    "\n",
    "We are interested in estimating the joint posterior of $\\lambda_1, \\lambda_2$ and $\\tau$ given the array of annnual disaster counts $\\mathbf{y}$. This gives:\n",
    "\n",
    "$$\n",
    " P( \\lambda_1, \\lambda_2, \\tau | \\mathbf{y} ) \\propto P(\\mathbf{y} | \\lambda_1, \\lambda_2, \\tau ) P(\\lambda_1, \\lambda_2, \\tau) \n",
    "$$\n",
    "\n",
    "To employ Gibbs sampling, we need to factor the joint posterior into the product of conditional expressions:\n",
    "\n",
    "$$\n",
    " P( \\lambda_1, \\lambda_2, \\tau | \\mathbf{y} ) \\propto P(y_{t<\\tau} | \\lambda_1, \\tau) P(y_{t\\ge \\tau} | \\lambda_2, \\tau) P(\\lambda_1) P(\\lambda_2) P(\\tau)\n",
    "$$\n",
    "\n",
    "which we have specified as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P( \\lambda_1, \\lambda_2, \\tau | \\mathbf{y} ) &\\propto \\left[\\prod_{t=1851}^{\\tau} \\text{Poi}(y_t|\\lambda_1) \\prod_{t=\\tau+1}^{1962} \\text{Poi}(y_t|\\lambda_2) \\right] \\text{Gamma}(\\lambda_1|\\alpha,\\beta) \\text{Gamma}(\\lambda_2|\\alpha, \\beta) \\frac{1}{111} \\\\\n",
    "&\\propto \\left[\\prod_{t=1851}^{\\tau} e^{-\\lambda_1}\\lambda_1^{y_t} \\prod_{t=\\tau+1}^{1962} e^{-\\lambda_2} \\lambda_2^{y_t} \\right] \\lambda_1^{\\alpha-1} e^{-\\beta\\lambda_1} \\lambda_2^{\\alpha-1} e^{-\\beta\\lambda_2} \\\\\n",
    "&\\propto \\lambda_1^{\\sum_{t=1851}^{\\tau} y_t +\\alpha-1} e^{-(\\beta+\\tau)\\lambda_1} \\lambda_2^{\\sum_{t=\\tau+1}^{1962} y_i + \\alpha-1} e^{-\\beta\\lambda_2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "So, the full conditionals are known, and critically for Gibbs, can easily be sampled from.\n",
    "\n",
    "$$\\lambda_1 \\sim \\text{Gamma}(\\sum_{t=1851}^{\\tau} y_t +\\alpha, \\tau+\\beta)$$\n",
    "$$\\lambda_2 \\sim \\text{Gamma}(\\sum_{t=\\tau+1}^{1962} y_i + \\alpha, 1962-\\tau+\\beta)$$\n",
    "$$\\tau \\sim \\text{Categorical}\\left( \\frac{\\lambda_1^{\\sum_{t=1851}^{\\tau} y_t +\\alpha-1} e^{-(\\beta+\\tau)\\lambda_1} \\lambda_2^{\\sum_{t=\\tau+1}^{1962} y_i + \\alpha-1} e^{-\\beta\\lambda_2}}{\\sum_{k=1851}^{1962} \\lambda_1^{\\sum_{t=1851}^{\\tau} y_t +\\alpha-1} e^{-(\\beta+\\tau)\\lambda_1} \\lambda_2^{\\sum_{t=\\tau+1}^{1962} y_i + \\alpha-1} e^{-\\beta\\lambda_2}} \\right)$$\n",
    "\n",
    "Implementing this in Python requires random number generators for both the gamma and discrete uniform distributions. We can leverage NumPy for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to draw random gamma variate\n",
    "rgamma = np.random.gamma\n",
    "\n",
    "def rcategorical(probs, n=None):\n",
    "    # Function to draw random categorical variate\n",
    "    return np.array(probs).cumsum().searchsorted(np.random.sample(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to generate probabilities for the conditional posterior of $\\tau$, we need the kernel of the gamma density:\n",
    "\n",
    "\\\\[\\lambda^{\\alpha-1} e^{-\\beta \\lambda}\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dgamma = lambda lam, a, b: lam**(a-1) * np.exp(-b*lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffuse hyperpriors for the gamma priors on $\\lambda_1, \\lambda_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha, beta = 1., 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational efficiency, it is best to pre-allocate memory to store the sampled values. We need 3 arrays, each with length equal to the number of iterations we plan to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify number of iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Initialize trace of samples\n",
    "lambda1, lambda2, tau = np.empty((3, n_iterations+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penultimate step initializes the model paramters to arbitrary values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda1[0] = 6\n",
    "lambda2[0] = 2\n",
    "tau[0] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample from conditionals\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # Sample early mean\n",
    "    lambda1[i+1] = rgamma(disasters_array[:tau[i]].sum() + alpha, 1./(tau[i] + beta))\n",
    "    \n",
    "    # Sample late mean\n",
    "    lambda2[i+1] = rgamma(disasters_array[tau[i]:].sum() + alpha, \n",
    "                          1./(n_count_data - tau[i] + beta))\n",
    "    \n",
    "    # Sample changepoint: first calculate probabilities (conditional)\n",
    "    p = np.array([dgamma(lambda1[i+1], disasters_array[:t].sum() + alpha, t + beta)*\n",
    "             dgamma(lambda2[i+1], \n",
    "                    disasters_array[t:].sum() + alpha, \n",
    "                    n_count_data - t + beta)\n",
    "             for t in range(n_count_data)])\n",
    "    # ... then draw sample\n",
    "    tau[i+1] = rcategorical(p/p.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the trace and histogram of the samples reveals the marginal posteriors of each parameter in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6355.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = '#3182bd'\n",
    "\n",
    "trace1 = Scatter(\n",
    "    y=lambda1,\n",
    "    xaxis='x1',\n",
    "    yaxis='y1',\n",
    "    line=Line(width=1),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=lambda1,\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    line=Line(width=0.5),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    y=lambda2,\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    line=Line(width=1),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace4 = Histogram(\n",
    "    x=lambda2,\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace5 = Scatter(\n",
    "    y=tau,\n",
    "    xaxis='x5',\n",
    "    yaxis='y5',\n",
    "    line=Line(width=1),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace6 = Histogram(\n",
    "    x=tau,\n",
    "    xaxis='x6',\n",
    "    yaxis='y6',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    "    \n",
    ")\n",
    "\n",
    "data = Data([trace1, trace2, trace3, trace4, trace5, trace6])\n",
    "fig = tls.make_subplots(3, 2)\n",
    "fig['data'] += data\n",
    "fig['layout'].update(showlegend=False, \n",
    "                     yaxis1=YAxis(\n",
    "                        title=r'$\\lambda_1$'\n",
    "                    ),\n",
    "                     yaxis3=YAxis(\n",
    "                        title=r'$\\lambda_2$'\n",
    "                    ),\n",
    "                     yaxis5=YAxis(\n",
    "                        title=r'$\\tau$'\n",
    "                    ),\n",
    ")\n",
    "\n",
    "py.iplot(fig, filename='traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metropolis-Hastings Algorithm\n",
    "\n",
    "The key to success in applying the Gibbs sampler to the estimation of Bayesian posteriors is being able to specify the form of the complete conditionals of\n",
    "${\\bf \\theta}$, because the algorithm cannot be implemented without them. In practice, the posterior conditionals cannot always be neatly specified. \n",
    "\n",
    "\n",
    "Taking a different approach, the Metropolis-Hastings algorithm generates ***candidate***  state transitions from an alternate distribution, and *accepts* or *rejects* each candidate probabilistically.\n",
    "\n",
    "Let us first consider a simple Metropolis-Hastings algorithm for a single parameter, $\\theta$. We will use a standard sampling distribution, referred to as the *proposal distribution*, to produce candidate variables $q_t(\\theta^{\\prime} | \\theta)$. That is, the generated value, $\\theta^{\\prime}$, is a *possible* next value for\n",
    "$\\theta$ at step $t+1$. We also need to be able to calculate the probability of moving back to the original value from the candidate, or\n",
    "$q_t(\\theta | \\theta^{\\prime})$. These probabilistic ingredients are used to define an *acceptance ratio*:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\begin{split}a(\\theta^{\\prime},\\theta) = \\frac{q_t(\\theta^{\\prime} | \\theta) \\pi(\\theta^{\\prime})}{q_t(\\theta | \\theta^{\\prime}) \\pi(\\theta)}\\end{split}\\notag\\\\\\begin{split}\\end{split}\\notag\\end{gathered}$$\n",
    "\n",
    "The value of $\\theta^{(t+1)}$ is then determined by:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\left\\{\\begin{array}{l@{\\quad \\mbox{with prob.} \\quad}l}\\theta^{\\prime} & \\text{with probability } \\min(a(\\theta^{\\prime},\\theta^{(t)}),1) \\\\ \\theta^{(t)} & \\text{with probability } 1 - \\min(a(\\theta^{\\prime},\\theta^{(t)}),1) \\end{array}\\right.$$\n",
    "\n",
    "This transition kernel implies that movement is not guaranteed at every step. It only occurs if the suggested transition is likely based on the acceptance ratio.\n",
    "\n",
    "A single iteration of the Metropolis-Hastings algorithm proceeds as follows:\n",
    "\n",
    "1.  Sample $\\theta^{\\prime}$ from $q(\\theta^{\\prime} | \\theta^{(t)})$.\n",
    "\n",
    "2.  Generate a Uniform[0,1] random variate $u$.\n",
    "\n",
    "3.  If $a(\\theta^{\\prime},\\theta) > u$ then\n",
    "    $\\theta^{(t+1)} = \\theta^{\\prime}$, otherwise\n",
    "    $\\theta^{(t+1)} = \\theta^{(t)}$.\n",
    "\n",
    "The original form of the algorithm specified by Metropolis required that\n",
    "$q_t(\\theta^{\\prime} | \\theta) = q_t(\\theta | \\theta^{\\prime})$, which reduces $a(\\theta^{\\prime},\\theta)$ to\n",
    "$\\pi(\\theta^{\\prime})/\\pi(\\theta)$, but this is not necessary. In either case, the state moves to high-density points in the distribution with high probability, and to low-density points with low probability. After convergence, the Metropolis-Hastings algorithm describes the full target posterior density, so all points are recurrent.\n",
    "\n",
    "### Random-walk Metropolis-Hastings\n",
    "\n",
    "A practical implementation of the Metropolis-Hastings algorithm makes use of a random-walk proposal.\n",
    "Recall that a random walk is a Markov chain that evolves according to:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} + \\epsilon_t \\\\\n",
    "\\epsilon_t \\sim f(\\phi)\n",
    "$$\n",
    "\n",
    "As applied to the MCMC sampling, the random walk is used as a proposal distribution, whereby dependent proposals are generated according to:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\begin{split}q(\\theta^{\\prime} | \\theta^{(t)}) = f(\\theta^{\\prime} - \\theta^{(t)}) = \\theta^{(t)} + \\epsilon_t\\end{split}\\notag\\\\\\begin{split}\\end{split}\\notag\\end{gathered}$$\n",
    "\n",
    "Generally, the density generating $\\epsilon_t$ is symmetric about zero,\n",
    "resulting in a symmetric chain. Chain symmetry implies that\n",
    "$q(\\theta^{\\prime} | \\theta^{(t)}) = q(\\theta^{(t)} | \\theta^{\\prime})$,\n",
    "which reduces the Metropolis-Hastings acceptance ratio to:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\begin{split}a(\\theta^{\\prime},\\theta) = \\frac{\\pi(\\theta^{\\prime})}{\\pi(\\theta)}\\end{split}\\notag\\\\\\begin{split}\\end{split}\\notag\\end{gathered}$$\n",
    "\n",
    "The choice of the random walk distribution for $\\epsilon_t$ is frequently a normal or Student’s $t$ density, but it may be any distribution that generates an irreducible proposal chain.\n",
    "\n",
    "An important consideration is the specification of the **scale parameter** for the random walk error distribution. Large values produce random walk steps that are highly exploratory, but tend to produce proposal values in the tails of the target distribution, potentially resulting in very small acceptance rates. Conversely, small values tend to be accepted more frequently, since they tend to produce proposals close to the current parameter value, but may result in chains that ***mix*** very slowly.\n",
    "\n",
    "Some simulation studies suggest optimal acceptance rates in the range of 20-50%. It is often worthwhile to optimize the proposal variance by iteratively adjusting its value, according to observed acceptance rates early in the MCMC simulation ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear model estimation\n",
    "\n",
    "This very simple dataset is a selection of real estate prices \\\\(p\\\\), with the associated age \\\\(a\\\\) of each house. We wish to estimate a simple linear relationship between the two variables, using the Metropolis-Hastings algorithm.\n",
    "\n",
    "**Linear model**:\n",
    "\n",
    "$$\\mu_i = \\beta_0 + \\beta_1 a_i$$\n",
    "\n",
    "**Sampling distribution**:\n",
    "\n",
    "$$p_i \\sim N(\\mu_i, \\tau)$$\n",
    "\n",
    "**Prior distributions**:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\beta_i \\sim N(0, 10000) \\cr\n",
    "& \\tau \\sim \\text{Gamma}(0.001, 0.001)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age = np.array([13, 14, 14,12, 9, 15, 10, 14, 9, 14, 13, 12, 9, 10, 15, 11, \n",
    "                15, 11, 7, 13, 13, 10, 9, 6, 11, 15, 13, 10, 9, 9, 15, 14, \n",
    "                14, 10, 14, 11, 13, 14, 10])\n",
    "\n",
    "price = np.array([2950, 2300, 3900, 2800, 5000, 2999, 3950, 2995, 4500, 2800, \n",
    "                  1990, 3500, 5100, 3900, 2900, 4950, 2000, 3400, 8999, 4000, \n",
    "                  2950, 3250, 3950, 4600, 4500, 1600, 3900, 4200, 6500, 3500, \n",
    "                  2999, 2600, 3250, 2500, 2400, 3990, 4600, 450,4700])/1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid numerical underflow issues, we typically work with log-transformed likelihoods, so the joint posterior can be calculated as sums of log-probabilities and log-likelihoods.\n",
    "\n",
    "This function calculates the joint log-posterior, conditional on values for each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import distributions\n",
    "dgamma = distributions.gamma.logpdf\n",
    "dnorm = distributions.norm.logpdf\n",
    "\n",
    "def calc_posterior(a, b, t, y=price, x=age):\n",
    "    # Calculate joint posterior, given values for a, b and t\n",
    "\n",
    "    # Priors on a,b\n",
    "    logp = dnorm(a, 0, 10000) + dnorm(b, 0, 10000)\n",
    "    # Prior on t\n",
    "    logp += dgamma(t, 0.001, 0.001)\n",
    "    # Calculate mu\n",
    "    mu = a + b*x\n",
    "    # Data likelihood\n",
    "    logp += sum(dnorm(y, mu, t**-2))\n",
    "    \n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metropolis` function implements a simple random-walk Metropolis-Hastings sampler for this problem. It accepts as arguments:\n",
    "\n",
    "- the number of iterations to run\n",
    "- initial values for the unknown parameters\n",
    "- the variance parameter of the proposal distribution (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnorm = np.random.normal\n",
    "runif = np.random.rand\n",
    "\n",
    "def metropolis(n_iterations, initial_values, prop_var=1):\n",
    "\n",
    "    n_params = len(initial_values)\n",
    "            \n",
    "    # Initial proposal standard deviations\n",
    "    prop_sd = [prop_var]*n_params\n",
    "    \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "    \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "        \n",
    "    # Calculate joint posterior for initial values\n",
    "    current_log_prob = calc_posterior(*trace[0])\n",
    "    \n",
    "    # Initialize acceptance counts\n",
    "    accepted = [0]*n_params\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "    \n",
    "        # Grab current parameter values\n",
    "        current_params = trace[i]\n",
    "    \n",
    "        for j in range(n_params):\n",
    "    \n",
    "            # Get current value for parameter j\n",
    "            p = trace[i].copy()\n",
    "    \n",
    "            # Propose new value\n",
    "            if j==2:\n",
    "                # Ensure tau is positive\n",
    "                theta = np.exp(rnorm(np.log(current_params[j]), prop_sd[j]))\n",
    "            else:\n",
    "                theta = rnorm(current_params[j], prop_sd[j])\n",
    "            \n",
    "            # Insert new value \n",
    "            p[j] = theta\n",
    "    \n",
    "            # Calculate log posterior with proposed value\n",
    "            proposed_log_prob = calc_posterior(*p)\n",
    "    \n",
    "            # Log-acceptance rate\n",
    "            alpha = proposed_log_prob - current_log_prob\n",
    "    \n",
    "            # Sample a uniform random variate\n",
    "            u = runif()\n",
    "    \n",
    "            # Test proposed value\n",
    "            if np.log(u) < alpha:\n",
    "                # Accept\n",
    "                trace[i+1,j] = theta\n",
    "                current_log_prob = proposed_log_prob\n",
    "                accepted[j] += 1\n",
    "            else:\n",
    "                # Reject\n",
    "                trace[i+1,j] = trace[i,j]\n",
    "                \n",
    "    return trace, accepted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the MH algorithm with a very small proposal variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10000\n",
    "trace, acc = metropolis(n_iter, initial_values=(1,0,1), prop_var=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the acceptance rate is way too high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.981 ,  0.9719,  0.966 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc, float)/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6355.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    y=trace.T[0],\n",
    "    xaxis='x1',\n",
    "    yaxis='y1',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=trace.T[0],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    y=trace.T[1],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace4 = Histogram(\n",
    "    x=trace.T[1],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace5 = Scatter(\n",
    "    y=trace.T[2],\n",
    "    xaxis='x5',\n",
    "    yaxis='y5',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace6 = Histogram(\n",
    "    x=trace.T[2],\n",
    "    xaxis='x6',\n",
    "    yaxis='y6',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    "    \n",
    ")\n",
    "data = Data([trace1, trace2, trace3, trace4, trace5, trace6])\n",
    "fig = tls.make_subplots(3, 2)\n",
    "fig['data'] += data\n",
    "fig['layout'].update(showlegend=False, \n",
    "                     yaxis1=YAxis(\n",
    "                        title='intercept'\n",
    "                    ),\n",
    "                     yaxis3=YAxis(\n",
    "                        title='slope'\n",
    "                    ),\n",
    "                     yaxis5=YAxis(\n",
    "                        title='precision'\n",
    "                    ),\n",
    ")\n",
    "\n",
    "py.iplot(fig, filename='traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a very large proposal variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n"
     ]
    }
   ],
   "source": [
    "trace_hivar, acc = metropolis(n_iter, initial_values=(1,0,1), prop_var=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0031,  0.0002,  0.0008])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc, float)/n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6355.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    y=trace_hivar.T[0],\n",
    "    xaxis='x1',\n",
    "    yaxis='y1',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=trace_hivar.T[0],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    y=trace_hivar.T[1],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace4 = Histogram(\n",
    "    x=trace_hivar.T[1],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace5 = Scatter(\n",
    "    y=trace_hivar.T[2],\n",
    "    xaxis='x5',\n",
    "    yaxis='y5',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace6 = Histogram(\n",
    "    x=trace_hivar.T[2],\n",
    "    xaxis='x6',\n",
    "    yaxis='y6',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    "    \n",
    ")\n",
    "data = Data([trace1, trace2, trace3, trace4, trace5, trace6])\n",
    "fig = tls.make_subplots(3, 2)\n",
    "fig['data'] += data\n",
    "fig['layout'].update(showlegend=False, \n",
    "                     yaxis1=YAxis(\n",
    "                        title='intercept'\n",
    "                    ),\n",
    "                     yaxis3=YAxis(\n",
    "                        title='slope'\n",
    "                    ),\n",
    "                     yaxis5=YAxis(\n",
    "                        title='precision'\n",
    "                    ),\n",
    ")\n",
    "\n",
    "py.iplot(fig, filename='traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Metropolis\n",
    "\n",
    "In order to avoid having to set the proposal variance by trial-and-error, we can add some tuning logic to the algorithm. The following implementation of Metropolis-Hastings reduces proposal variances  by 10% when the acceptance rate is low, and increases it by 10% when the acceptance rate is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def metropolis_tuned(n_iterations, initial_values, f=calc_posterior, prop_var=1, \n",
    "                     tune_for=None, tune_interval=100):\n",
    "    \n",
    "    n_params = len(initial_values)\n",
    "            \n",
    "    # Initial proposal standard deviations\n",
    "    prop_sd = [prop_var] * n_params\n",
    "    \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "    \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "    # Initialize acceptance counts\n",
    "    accepted = [0]*n_params\n",
    "    \n",
    "    # Calculate joint posterior for initial values\n",
    "    current_log_prob = f(*trace[0])\n",
    "    \n",
    "    if tune_for is None:\n",
    "        tune_for = n_iterations/2\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "    \n",
    "        # Grab current parameter values\n",
    "        current_params = trace[i]\n",
    "    \n",
    "        for j in range(n_params):\n",
    "    \n",
    "            # Get current value for parameter j\n",
    "            p = trace[i].copy()\n",
    "    \n",
    "            # Propose new value\n",
    "            if j==2:\n",
    "                # Ensure tau is positive\n",
    "                theta = np.exp(rnorm(np.log(current_params[j]), prop_sd[j]))\n",
    "            else:\n",
    "                theta = rnorm(current_params[j], prop_sd[j])\n",
    "            \n",
    "            # Insert new value \n",
    "            p[j] = theta\n",
    "    \n",
    "            # Calculate log posterior with proposed value\n",
    "            proposed_log_prob = f(*p)\n",
    "    \n",
    "            # Log-acceptance rate\n",
    "            alpha = proposed_log_prob - current_log_prob\n",
    "    \n",
    "            # Sample a uniform random variate\n",
    "            u = runif()\n",
    "    \n",
    "            # Test proposed value\n",
    "            if np.log(u) < alpha:\n",
    "                # Accept\n",
    "                trace[i+1,j] = theta\n",
    "                current_log_prob = proposed_log_prob\n",
    "                accepted[j] += 1\n",
    "            else:\n",
    "                # Reject\n",
    "                trace[i+1,j] = trace[i,j]\n",
    "                \n",
    "            # Tune every 100 iterations\n",
    "            if (not (i+1) % tune_interval) and (i < tune_for):\n",
    "        \n",
    "                # Calculate aceptance rate\n",
    "                acceptance_rate = (1.*accepted[j])/tune_interval\n",
    "                if acceptance_rate<0.1:\n",
    "                    prop_sd[j] *= 0.9\n",
    "                if acceptance_rate<0.2:\n",
    "                    prop_sd[j] *= 0.95\n",
    "                if acceptance_rate>0.4:\n",
    "                    prop_sd[j] *= 1.05\n",
    "                elif acceptance_rate>0.6:\n",
    "                    prop_sd[j] *= 1.1\n",
    "        \n",
    "                accepted[j] = 0\n",
    "                \n",
    "    return trace[tune_for:], accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n",
      "Iteration 10000\n",
      "Iteration 11000\n",
      "Iteration 12000\n",
      "Iteration 13000\n",
      "Iteration 14000\n",
      "Iteration 15000\n",
      "Iteration 16000\n",
      "Iteration 17000\n",
      "Iteration 18000\n",
      "Iteration 19000\n"
     ]
    }
   ],
   "source": [
    "trace_tuned, acc = metropolis_tuned(n_iter*2, initial_values=(1,0,1), prop_var=5, tune_interval=25, tune_for=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2804,  0.2902,  0.2766])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc, float)/(n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6355.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    y=trace_tuned.T[0],\n",
    "    xaxis='x1',\n",
    "    yaxis='y1',\n",
    "    line=Line(width=1),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=trace_tuned.T[0],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    y=trace_tuned.T[1],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    line=Line(width=1),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace4 = Histogram(\n",
    "    x=trace_tuned.T[1],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace5 = Scatter(\n",
    "    y=trace_tuned.T[2],\n",
    "    xaxis='x5',\n",
    "    yaxis='y5',\n",
    "    line=Line(width=0.5),\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace6 = Histogram(\n",
    "    x=trace_tuned.T[2],\n",
    "    xaxis='x6',\n",
    "    yaxis='y6',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    "    \n",
    ")\n",
    "data = Data([trace1, trace2, trace3, trace4, trace5, trace6])\n",
    "fig = tls.make_subplots(3, 2)\n",
    "fig['data'] += data\n",
    "fig['layout'].update(showlegend=False, \n",
    "                     yaxis1=YAxis(\n",
    "                        title='intercept'\n",
    "                    ),\n",
    "                     yaxis3=YAxis(\n",
    "                        title='slope'\n",
    "                    ),\n",
    "                     yaxis5=YAxis(\n",
    "                        title='precision'\n",
    "                    ),\n",
    ")\n",
    "\n",
    "py.iplot(fig, filename='traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 random regression lines drawn from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6356.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data points\n",
    "points = Scatter(\n",
    "    x=age,\n",
    "    y=price,\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "# Sample models from posterior\n",
    "xvals = np.linspace(age.min(), age.max())\n",
    "line_data = [np.column_stack([np.ones(50), xvals]).dot(trace_tuned[np.random.randint(0, 1000), :2]) for i in range(50)]\n",
    "\n",
    "# Generate Scatter obejcts\n",
    "lines = [Scatter(x=xvals, y=line, opacity=0.5, \\\n",
    "                 marker=Marker(color='#e34a33'), line=Line(width=0.5)) for line in line_data]\n",
    "\n",
    "data = Data([points]+lines)\n",
    "\n",
    "layout = Layout(\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    xaxis=XAxis(\n",
    "        title='Age',\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=YAxis(\n",
    "        title='Price',\n",
    "        showline=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='regression_lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bioassay analysis\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is `death`, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "</div>\n",
    "\n",
    "The common statistic of interest in such experiments is the **LD50**, the dosage at which the probability of death is 50%.\n",
    "\n",
    "Use Metropolis-Hastings sampling to fit a Bayesian model to analyze this bioassay data, and to estimate LD50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-.86, -.3, -.05, .73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import distributions\n",
    "dbin = distributions.binom.logpmf\n",
    "dnorm = distributions.norm.logpdf\n",
    "\n",
    "invlogit = lambda x: 1./(1 + np.exp(-x))\n",
    "\n",
    "def calc_posterior(a, b, y=deaths, x=log_dose):\n",
    "\n",
    "    # Priors on a,b\n",
    "    logp = dnorm(a, 0, 10000) + dnorm(b, 0, 10000)\n",
    "    # Calculate p\n",
    "    p = invlogit(a + b*np.array(x))\n",
    "    # Data likelihood\n",
    "    logp += sum([dbin(yi, n, pi) for yi,pi in zip(y,p)])\n",
    "    \n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n"
     ]
    }
   ],
   "source": [
    "bioassay_trace, acc = metropolis_tuned(n_iter, f=calc_posterior, initial_values=(1,0), prop_var=5, tune_for=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~jackp/6355.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    y=bioassay_trace.T[0],\n",
    "    xaxis='x1',\n",
    "    yaxis='y1',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=bioassay_trace.T[0],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    y=bioassay_trace.T[1],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    ")\n",
    "trace4 = Histogram(\n",
    "    x=bioassay_trace.T[1],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    marker=Marker(\n",
    "        color=color)\n",
    "    \n",
    ")\n",
    "data = Data([trace1, trace2, trace3, trace4])\n",
    "fig = tls.make_subplots(2, 2)\n",
    "fig['data'] += data\n",
    "fig['layout'].update(showlegend=False, \n",
    "                     yaxis1=YAxis(\n",
    "                        title='intercept'\n",
    "                    ),\n",
    "                     yaxis3=YAxis(\n",
    "                        title='slope'\n",
    "                    ),\n",
    ")\n",
    "\n",
    "py.iplot(fig, filename='traces')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
